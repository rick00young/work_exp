# 主成分分析Principal component analysis(PCA)

[http://www.dataivy.cn/blog/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90principal-component-analysis_pca/?wt_tb=3%7C1503117432574](http://www.dataivy.cn/blog/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90principal-component-analysis_pca/?wt_tb=3%7C1503117432574)


> 从主观的理解上，主成分分析到底是什么？它其实是对数据在高维空间下的一个投影转换，通过一定的投影规则将原来从一个角度看到的多个维度映射成较少的维度。到底什么是映射，下面的图就可以很好地解释这个问题——正常角度看是两个半椭圆形分布的数据集，但经过旋转（映射）之后是两条线性分布数据集。
> 在面对海量数据或大数据进行数据挖掘时，通常会面临“维度灾难”，原因是数据集的维度可以不断增加而无穷多，但计算机的处理能力和速度却不是无限的；另外，数据集的大量维度之间可能存在共线性的关系，这会直接导致学习模型的健壮性不够，甚至很多时候会失效。因此，我们需要一种可以降低维度数量并降低维度间共线性影响的方法——这就是降维的意义所在。

> 主成分分析是一种降维方法。主成分分析Principal component analysis(PCA)也称主分量分析，旨在利用降维的思想，把多维指标转化为少数几个综合维度，然后利用这些综合维度进行数据挖掘和学习，以代替原来利用所有维度进行挖掘学习的方法。


> 主成分分析的基本方法是按照一定的数学变换方法，把给定的一组相关变量（维度）通过线性变换转成另一组不相关的变量，这些新的变量按照方差依次递减的顺序排列。在数学变换中保持变量的总方差不变，使第一变量具有最大的方差，称为第一主成分，第二变量的方差次大，并且和第一变量不相关，称为第二主成分。依次类推，I个变量就有I个主成分。

```
#coding:utf-8   
  
  
from sklearn import datasets   
from sklearn.decomposition import PCA   
  
iris = datasets.load_iris()   
X = iris.data   
print ('first 10 raw samples:', X[:10])   
pca = PCA(n_components=2)   
X_r = pca.fit(X).transform(X)   
print ('first 10 transformed samples:', X_r[:10])   
print ('variance:',pca.explained_variance_ratio_)   
```

应用场景：

* 如本文开始所说的，PCA的应用是降维，用在所有大量数据集建模处理之前的降维过程，因此它是数据预处理过程的一步。

# 线性判别分析Linear Discriminant Analysis (LDA)

[http://www.dataivy.cn/blog/%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90linear-discriminant-analysis_lda/?wt_tb=9%7C1503125123559](http://www.dataivy.cn/blog/%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90linear-discriminant-analysis_lda/?wt_tb=9%7C1503125123559)

> 基本思想是将高维的模式样本投影到最佳鉴别矢量空间，以达到抽取分类信息和压缩特征空间维数的效果，投影后保证模式样本在新的子空间有最大的类间距离和最小的类内距离，即模式在该空间中有最佳的可分离性。


在文章《主成分分析Principal component analysis(PCA)》中曾介绍过数据降维的主成分分析(PCA)，LDA与PCA都是常用的降维方法，二者的区别在于：

> 出发思想不同。PCA主要是从特征的协方差角度，去找到比较好的投影方式，即选择样本点投影具有最大方差的方向；而LDA则更多的是考虑了分类标签信息，寻求投影后不同类别之间数据点距离更大化以及同一类别数据点距离最小化，即选择分类性能最好的方向。

>学习模式不同。PCA属于无监督式学习，因此大多场景下只作为数据处理过程的一部分，需要与其他算法结合使用，例如将PCA与聚类、判别分析、回归分析等组合使用；LDA是一种监督式学习方法，本身除了可以降维外，还可以进行预测应用，因此既可以组合其他模型一起使用，也可以独立使用。

> 降维后可用维度数量不同。LDA降维后最多可生成C-1维子空间（分类标签数-1），因此LDA与原始维度数量无关，只有数据标签分类数量有关；而PCA最多有n维度可用，即最大可以选择全部可用维度。
> 

当样本数量远小于样本的特征维数，样本与样本之间的距离变大使得距离度量失效，使LDA算法中的类内、类间离散度矩阵奇异，不能得到最优的投影方向，在人脸识别领域中表现得尤为突出

>LDA不适合对非高斯分布的样本进行降维

>LDA在样本分类信息依赖方差而不是均值时，效果不好

>LDA可能过度拟合数据

```
#coding:utf-8   
from sklearn import datasets
from sklearn.lda import LDA
iris = datasets.load_iris()
X = iris.data[:-5]
pre_x = iris.data[-5:]
y = iris.target[:-5]
print ('first 10 raw samples:', X[:10])
clf = LDA()
clf.fit(X, y)
X_r = clf.transform(X)
pre_y = clf.predict(pre_x)
#降维结果
print ('first 10 transformed samples:', X_r[:10])
#预测目标分类结果
print ('predict value:', pre_y)
```

**LDA是是一个经典的机器学习算法，它是判别分析中的线性分类器，在很多应用情况下会面临数据稀疏的问题，尤其是在面部识别的场景：数据的维度很可能大于数据的样本量，甚至可能呈几倍的差异。此时，LDA的预测准确率会表现较差，当维度数/样本量达到4倍时，准确率会只有50%左右，解决方法之一是可以对LDA算法进行收缩，Python的SKlearn中的LDA算法支持这一收缩规则。默认情况下，solver的值被设定为“svd”，这在大数据量下的表现很好，但不支持收缩规则；当面临数据稀疏时，我们需要使用“lsqr”或“eigen”，另外，与之配合的是shrinkage参数需要设置成auto以便于算法自动调整收缩值，当然你也可以自己凭借经验将值设定在0~1之间（越大收缩越厉害：0时不收缩，1时意味着对角线方差矩阵将被用作协方差矩阵值的估计）。**

LDA的应用应用场景：

* 人脸识别中的降维或模式识别
* 根据市场宏观经济特征进行经济预测
* 根据市场或用户不同属性进行市场调研
* 根据患者病例特征进行医学病情预测

# 独立成分分析Independent component analysis(ICA)

[http://www.dataivy.cn/blog/%E7%8B%AC%E7%AB%8B%E6%88%90%E5%88%86%E5%88%86%E6%9E%90independent-component-analysis_ica/?wt_tb=9%7C1503125123558](http://www.dataivy.cn/blog/%E7%8B%AC%E7%AB%8B%E6%88%90%E5%88%86%E5%88%86%E6%9E%90independent-component-analysis_ica/?wt_tb=9%7C1503125123558)

> 传统的降维方法，包括PCA、LDA等都是以观测数据点呈高斯分布模型为基本假设前提的，在已经先验经验知道观测数据集为非高斯分布模型的前提下，PCA和LDA的降维效果并不好；而本文介绍的ICA将适用于非高斯分析数据集，它是CIA，是主成分分析（PCA）和因子分析（Factor Analysis）的一种有效扩展。

独立成分分析（Independent component analysis，简称ICA）是一种利用统计原理进行计算的方法，它是一个线性变换，这个变换把数据或信号分离成统计独立的非高斯的信号源的线性组合。


独立成分分析的最重要的假设就是信号源统计独立，并且这个假设在大多数盲信号分离（blind signal separation）的情况中符合实际情况；但即使当该假设不满足时，仍然可以用独立成分分析来把观察信号统计独立化，从而进一步分析数据的特性。

ICA应用前提很简单：数据信号源是独立的且数据非高斯分布（或者信号源中最多只有一个成分是高斯分布），另外观测信号源的数目不能少于源信号数目（为了方便一般要求二者相等即可）。

```
#coding:utf-8
import numpy as np
import matplotlib.pyplot as plt
from scipy import signal
from sklearn.decomposition import FastICA, PCA
# 生成观测模拟数据
np.random.seed(0)
n_samples = 2000
time = np.linspace(0, 8, n_samples)
s1 = np.sin(2 * time)  # 信号源 1 : 正弦信号
s2 = np.sign(np.sin(3 * time))  # 信号源 2 : 方形信号
s3 = signal.sawtooth(2 * np.pi * time)  # 信号源 3: 锯齿波信号
S = np.c_[s1, s2, s3]
S += 0.2 * np.random.normal(size=S.shape)  # 增加噪音数据
S /= S.std(axis=0)  # 标准化
# 混合数据
A = np.array([[1, 1, 1], [0.5, 2, 1.0], [1.5, 1.0, 2.0]])  # 混合矩阵
X = np.dot(S, A.T)  # 生成观测信号源
# ICA模型
ica = FastICA(n_components=3)
S_ = ica.fit_transform(X)  # 重构信号
A_ = ica.mixing_  # 获得估计混合后的矩阵
# PCA模型
pca = PCA(n_components=3)
H = pca.fit_transform(X)  # 基于PCA的成分正交重构信号源
# 图形展示
plt.figure()
models = [X, S, S_, H]
names = ['Observations (mixed signal)',
         'True Sources',
         'ICA recovered signals',
         'PCA recovered signals']
colors = ['red', 'steelblue', 'orange']
for ii, (model, name) in enumerate(zip(models, names), 1):
    plt.subplot(4, 1, ii)
    plt.title(name)
    for sig, color in zip(model.T, colors):
        plt.plot(sig, color=color)
plt.subplots_adjust(0.09, 0.04, 0.94, 0.94, 0.26, 0.46)
plt.show()
```

由结果可知，原始观测信号源有3种独立信号源混合（正弦、方形和锯齿波形），通过ICA可以较好的分离出三种信号源，而PCA的分离效果不佳，只分离出一个信号源。

ICA的经典问题是“鸡尾酒会问题”（cocktail party problem），这个问题假设在party（或鸡尾酒会）中有n个人，我们利用房间里面的n个麦克风或录音机对同时说话的这些人进行录音，由此得到一个混合信号源或声音源，现在要做的就是从这个信号源中分离出不同人的说话声音。

独立成分分析法（ICA）最初是用来解决“鸡尾酒会”的问题，ICA基于信号高阶统计特性的分析方法，经ICA分解出的各信号成分（或者叫分量）之间是相互独立的，正是因为这一特点，ICA在信号处理领域受到了广泛的关注。除了经典的盲源分离外，它的应用领域还包括：

* 图像识别，去除噪音信息
* 语言识别，分离音源并去除噪音（如去除噪音只保留输入语音）
* 通信、生物医学信号处理，从这些信号中单独区分某些信号（如区分胎儿和孕妇的心电信号）
* 故障诊断，去除非自然信息
* 特征提取和降维
* 自然信息处理，如地震声音分离


盲源分离中的盲源指的是源信号不可观测且混合特性不可知，很多时候盲源分离（blind signal separation）会被称为独立成分分析，但其实仅当盲源中的不同信号源相互独立时，盲源分离才与独立成分分析等价。因此，独立成分分析是盲源分离的一种特例。因为很多时候，混合信号源是不独立的。

# 二次判别分析Quadratic Discriminant Analysis(QDA)

[http://www.dataivy.cn/blog/%E4%BA%8C%E6%AC%A1%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90quadratic-discriminant-analysis_qda/?wt_tb=9%7C1503125123558](http://www.dataivy.cn/blog/%E4%BA%8C%E6%AC%A1%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90quadratic-discriminant-analysis_qda/?wt_tb=9%7C1503125123558)

与线性判别分析类似，二次判别分析是另外一种线性判别分析算法，二者拥有类似的算法特征，区别仅在于：当不同分类样本的协方差矩阵相同时，使用线性判别分析；当不同分类样本的协方差矩阵不同时，则应该使用二次判别。关于线性判别分析的更多内容，请在文章《线性判别分析Linear Discriminant Analysis (LDA)》中查看。


那么，协方差矩阵是什么？

在统计学中，有几个描述样本分布的基本指标，例如均值、方差、标准差、峰度、偏度、最大值、最小值、极值等，这些都描述的是一个维度；如果一个样本存在多个维度，除了可以单独描述每个维度的分布规律外，如何描述不同维度间的关系？

协方差就是用来描述维度间关系的一个指标。它的定义为：任意两个随机变量X和Y的协方差，记为Cov(X,Y)，定义为：

Cov(X,Y)=E{[ X-E(X)][Y-E(Y) ]}

其中E(X)、E(Y)反映分量X、Y各自的均值。它反映的是任意两个随机变量（或者是任意两个维度） 间的关系：

当X取值不断增大时，Y也不断增大，此时Cov(X,Y)>0

当X取值不断减小时，Y也不断减小，此时Cov(X,Y)>0

当X取值不断增大时，Y也不断减小，此时Cov(X,Y)<0

当X取值不断减小时，Y也不断增大，此时Cov(X,Y)<0

总结X和Y之间的规律就是，当两个随机变量倾向于沿着相同趋势变化时为正协方差，反之则为负协方差。

但我们知道，不同维度间可能由于值本身存在量级的差异而导致结果的偏差，例如订单金额的单位可能是万元区间，而用户等级可能只是10以内的数字分布。为了消除这个差异性，需要对协方差进行标准化，而标准化后的指标即相关系数（通常用P表示），其矩阵被称为相关性矩阵。


**相关系统P的基本意义如下：**

**当P>0时，X和Y呈正相关**

**当P<0时，X和Y呈负相关**

```
*#coding:utf-8  * 
from sklearn import datasets
from sklearn.qda import QDA
iris = datasets.load_iris()
X = iris.data[:-5]
pre_x = iris.data[-5:]
y = iris.target[:-5]
clf = QDA()
clf.fit(X, y)
pre_y = clf.predict(pre_x)
#预测目标分类结果
print ('predict value:', pre_y)
```

# 因子分析(Factor Analysis)

[http://www.dataivy.cn/blog/%E5%9B%A0%E5%AD%90%E5%88%86%E6%9E%90factor-analysis/?wt_tb=1%7C1503124858338](http://www.dataivy.cn/blog/%E5%9B%A0%E5%AD%90%E5%88%86%E6%9E%90factor-analysis/?wt_tb=1%7C1503124858338)

因子分析（Factor Analysis）是指研究从变量群中提取共性因子的统计技术，这里的共性因子指的是不同变量之间内在的隐藏因子。例如，一个学生的英语、数据、语文成绩都很好，那么潜在的共性因子可能是智力水平高。因此，因子分析的过程其实是寻找共性因子和个性因子并得到最优解释的过程。


因子分析有两个核心问题：一是如何构造因子变量，二是如何对因子变量进行命名解释。因子分析有下面4个基本步骤：

1. 确定原有若干变量是否适合于因子分析。因子分析的基本逻辑是从原始变量中构造出少数几个具有代表意义的因子变量，这就要求原有变量之间要具有比较强的相关性，否则，因子分析将无法提取变量间的“共性特征”（变量间没有共性还如何提取共性？）。实际应用时，可以使用相关性矩阵进行验证，如果相关系数小于0.3，那么变量间的共性较小，不适合使用因子分析。
2. 构造因子变量。因子分析中有多种确定因子变量的方法，如基于主成分模型的主成分分析法和基于因子分析模型的主轴因子法、极大似然法、最小二乘法等。其中基于主成分模型的主成分分析法是使用最多的因子分析方法之一。
3. 利用旋转使得因子变量更具有可解释性。在实际分析工作中，主要是因子分析得到因子和原变量的关系，从而对新的因子能够进行命名和解释，否则其不具有可解释性的前提下对比PCA就没有明显的可解释价值。
4. 计算因子变量的得分。计算因子得分是因子分析的最后一步，因子变量确定以后，对每一样本数据，希望得到它们在不同因子上的具体数据值，这些数值就是因子得分，它和原变量的得分相对应。
在上述因子分析中，已经阐明了因子变量的可解释性是因子分析的核心问题之一（实际上从应用角度来讲，也是比PCA更为有效的应用条件之一），那么什么是可解释性？我们看下面因子旋转成分矩阵。


因子分析经常与主成分分析（PCA）进行对比，以下是二者之间的异同点。

主要相同点：

PCA和因子分析都是数据降维的重要方法，都对原始数据进行标准化处理，都消除了原始指标的相关性对综合评价所造成的信息重复的影响；二者构造综合评价时所涉及的权数具有客观性，在原始信息损失不大的前提下，减少了后期数据挖掘和分析的工作量。

> 之所以大多数情况下，很难感性的区分因子分析和主成分分析，原因是二者的降维结果都是对原有维度进行一定的处理，在处理的结果上都偏离了原有基于维度的认识；但只要清楚二者的逻辑一个是基于变量的线性组合，一个是基于因子的组合便能很好的进行区分。另外，也可以从以下几个角度进行区别：工作原理、假设条件、求解方法以及降维后的特征。

主要区别：

* 原理不同。主成分分析的基本原理是利用降维（线性变换)的思想，在损失很少信息的前提下把多个指标转化为几个不相关的主成分，每个主成分都是原始变量的线性组合；
而因子分析基本原理是从原始变量相关矩阵内部的依赖关系出发，把因子表达成能表示成少数公共因子和仅对某一个变量有作用的特殊因子的线性组合。（因子分析是主成分的推广，相对于主成分分析，更倾向于描述原始变量之间的相关关系）
* 假设条件不同。主成分分析不需要有假设，而因子分析需要假设各个共同因子之间不相关，特殊因子（specificfactor）之间也不相关，共同因子和特殊因子之间也不相关。
* 求解方法不同。主成分分析的求解方法从协方差阵出发，而因子分析的求解方法包括主成分法、主轴因子法、极大似然法、最小二乘法、a因子提取法等。
* 降维后的“维度”数量不同，即因子数量和主成分的数量。主成分分析的数量最多等于维度数；而因子分析中的因子个数需要分析者指定（SPSS和SAS根据一定的条件自动设定，只要是特征值大于1的因子主可进入分析），指定的因子数量不同而结果也不同。

综合来看，因子分析在实现中可以使用旋转技术，因此可以得到更好的因子解释，这一点比主成分占优势；另外，因子分析不需要舍弃原有变量，而是站到原有变量间的共性因子作为下一步应用的前提，其实就是由表及里去发现内在规律。但是，主成分分析由于不需要假设条件，并且可以最大限度的保持原有变量的大多数特征，因此适用范围更广泛，尤其是宏观的未知数据的稳定度更高。

因子分析跟主成分分析一样，由于侧重点都是进行数据降维，因此很少单独使用，大多数情况下都会有一些模型组合使用。例如：

* 因子分析（主成分分析）+多元回归分析，判断并解决共线性问题之后进行回归预测；
* 因子分析（主成分分析）+聚类分析，通过降维后的数据进行聚类并分析数据特点，但因子分析会更适合，原因是基于因子的聚类结果更容易解释，而基于主成分的聚类结果很难解释。
* 因子分析（主成分分析）+分类，数据降维（或数据压缩）后进行分类预测，这也是常用的组合方法。

```
#download_link = http://www.dataivy.cn/upload/abalone.txt?wt_tb=13%7C1503126405196
#coding:utf-8
import re
from os.path import join
from os.path import dirname
from prettytable import PrettyTable
from sklearn.decomposition import FactorAnalysis
import numpy as np
filename = 'abalone.txt'
req = re.compile(r'\t|,|;|!')#自定义分隔符
data_file_name = join(dirname(__file__),filename)
with open(data_file_name) as fr:
    numberOfLines = len(fr.readlines())
returnMat = []
with open(data_file_name) as fr:
    for line in fr.readlines():
        line = line.strip()
        listFromLine = req.split(line)
        returnMat.append(listFromLine)
table = PrettyTable(['Key','Value'])
table.align['Key'] = '1'
table.add_row(['Sample number:', numberOfLines-1])
table.add_row(['Feature number:',len(returnMat[0])])
table.add_row(['feature name:', returnMat[0]])
table.add_row(['classLabelVector name:',None])
table.padding_width = 1
#table.sort_key('Key')
print (table)
data = np.array(returnMat[1:], dtype = np.float64)
fa = FactorAnalysis(n_components = 3)
fa.fit(data)
tran_data = fa.transform(data)
print ('transform data:',tran_data[:10:])
```

主成分分析和因子分析是数据降维过程中使用频率相对较高的两类降维方法，具体使用时如果后续数据挖掘或处理过程需要解释或者通过原始变量的意义去应用，那么选择因子分析更合适，否则主成分分析的使用场景广泛。但我们发现，其实主成分分析可以看做是因子分析的一个特例，而且主成分分析也可以作为因子分析的一种方法。


# 数据预处理-清洗转换


***

1. 纠正错误

错误数据是数据源环境中经常出现的一类问题。数据错误的形式包括：

数据值错误：数据直接是错误的，例如超过固定域集、超过极值、拼写错误、属性错误、源错误等。
数据类型错误：数据的存储类型不符合实际情况，如日期类型的以数值型存储，时间戳存为字符串等。
数据编码错误：数据存储的编码错误，例如将UTF-8写成UTF-80。
数据格式错误：数据的存储格式问题，如半角全角字符、中英文字符等。
数据异常错误：如数值数据输成全角数字字符、字符串数据后面有一个回车操作、日期越界、数据前后有不可见字符等。
依赖冲突：某些数据字段间存储依赖关系，例如城市与邮政编码应该满足对应关系，但可能存在二者不匹配的问题。
多值错误：大多数情况下，每个字段存储的是单个值，但也存在一个字段存储多个值的情况，其中有些可能是不符合实际业务规则的。
这类错误产生的原因是业务系统不够健全，尤其是在数据产生之初的校验和入库规则不规范，导致在接收输入后没有进行判断或无法检测而直接写入后台数据库造成的。

2. 删除重复项

由于各种原因，数据中可能存在重复记录或重复字段（列），对于这些重复项目（行和列）需要做去重处理。

对于重复项的判断，基本思想是“排序和合并”，先将数据库中的记录按一定规则排序，然后通过比较邻近记录是否相似来检测记录是否重复。这里面其实包含了两个操作，一是排序，二是计算相似度。

常见的排序算法：

```
插入排序
冒泡排序
选择排序
快速排序
堆排序
归并排序
基数排序
希尔排序
常见的判断相似度的算法：

基本的字段匹配算法
标准化欧氏距离
汉明距离
夹角余弦
杰卡德距离
马氏距离
曼哈顿距离
闵可夫斯基距离
欧氏距离
切比雪夫距离
相关系数
信息熵
```
对于重复的数据项，尽量需要经过业务确认并进行整理提取出规则。在清洗转换阶段，对于重复数据项尽量不要轻易做出删除决策，尤其不能将重要的或有业务意义的数据过滤掉，校验和重复确认的工作必不可少。

3. 统一规格

由于数据源系统分散在各个业务线，不同业务线对于数据的要求、理解和规格不同，导致对于同一数据对象描述规格完全不同，因此在清洗过程中需要统一数据规格并将一致性的内容抽象出来。

数据字段的规则大致可以从以下几个方面进行统一：

* 名称，对于同一个数据对象的名称首先应该是一致的。例如对于访问深度这个字段，可能的名称包括访问深度、人均页面浏览量、每访问PV数。
* 类型：同一个数据对象的数据类型必须统一，且表示方法一致。例如普通日期的类型和时间戳的类型需要区分。
* 单位：对于数值型字段，单位需要统一。例如万、十万、百万等单位度量。
* 格式：在同一类型下，不同的表示格式也会产生差异。例如日期中的长日期、短日期、英文、中文、年月日制式和缩写等格式均不一样。
* 长度：同一字段长度必须一致。
* 小数位数：小数位数对于数值型字段尤为重要，尤其当数据量累积较大时会因为位数的不同而产生巨大偏差。
* 计数方法：对于数值型等的千分位、科学计数法等的计数方法的统一。
* 缩写规则：对于常用字段的缩写，例如单位、姓名、日期、月份等的统一。例如将周一表示为Monday还是Mon还是M。
* 值域：对于离散型和连续型的变量都应该根据业务规则进行统一的值域约束。
* 约束：是否允许控制、唯一性、外键约束、主键等的统一。

统一数据规格的过程中，需要重要的一点是确认不同业务线带来数据的规格一致性，这需要业务部门的参与、讨论和确认，以明确不同体系数据的统一标准。

4. 修正逻辑

在多数据源的环境下，很可能存在数据异常或冲突的问题。

例如不同的数据源对于订单数量的数据统计冲突问题，结果出现矛盾的记录。通常，这是由于不同系统对于同一个数据对象的统计逻辑不同而造成的，逻辑的不一致会直接导致结果的差异性；除了统计逻辑和口径的差异，也有因为源数据系统基于性能的考虑，放弃了外键约束，从而导致数据不一致的结果；另外，也存在极小的数据丢失的可能性，通常由于并发量和负载过高、服务器延迟甚至宕机等原因导致的数据采集的差异。

对于这类的数据矛盾，首先需要明确各个源系统的逻辑、条件、口径，然后定义一套符合各个系统采集逻辑的规则，并对异常源系统的采集逻辑进行修正。

某些情况下，也可能存在业务规则的错误导致的数据采集的错误，此时需要从源头纠正错误的采集逻辑，然后再进行数据清洗和转换。

5. 转换构造

数据变换是数据清理过程的重要步骤，是对数据的一个的标准的处理，几乎所有的数据处理过程都会涉及该步骤。数据转换常见的内容包括：数据类型转换、数据语义转换、数据值域转换、数据粒度转换、表/数据拆分、行列转换、数据离散化、数据离散化、提炼新字段、属性构造、数据压缩等。

数据类型转换

当数据来自不同数据源时，不同类型的数据源数据类型不兼容可能导致系统报错。这时需要将不同数据源的数据类型进行统一转换为一种兼容的数据类型。

数据语义转换

传统数据仓库中基于第三范式可能存在维度表、事实表等，此时在事实表中会有很多字段需要结合维度表才能进行语义上的解析。例如，假如字段M的业务含义是浏览器类型，其取值分为是1/2/3/4/5，这5个数字如果不加转换则很难理解为业务语言，更无法在后期被解读和应用。

数据粒度转换

业务系统一般存储的是明细数据，有些系统甚至存储的是基于时间戳的数据，而数据仓库中的数据是用来分析的，不需要非常明细的数据，一般情况下，会将业务系统数据按照数据仓库中不同的粒度需求进行聚合。

表/数据拆分

某些字段可能存储多中数据信息，例如时间戳中包含了年、月、日、小时、分、秒等信息，有些规则中需要将其中部分或者全部时间属性进行拆分，以此来满足多粒度下的数据聚合需求。同样的，一个表内的多个字段，也可能存在表字段拆分的情况。

行列转换

某些情况下，表内的行列数据会需要进行转换（又称为转置），例如协同过滤的计算之前，user和term之间的关系即互为行列并且可相互转换，可用来满足基于项目和基于用户的相似度推荐计算。

数据离散化

将连续取值的属性离散化成若干区间，来帮助消减一个连续属性的取值个数。例如对于收入这个字段，为了便于做统计，根据业务经验可能分为几个不同的区间：0～3000、3001～5000、5001～10000、10001～30000、大于30000，或者在此基础上分别用1、2、3、4、5来表示。

数据标准化

不同字段间由于字段本身的业务含义不同，有些时间需要消除变量之间不同数量级造成的数值之间的悬殊差异。例如将销售额进行离散化处理，以消除不同销售额之间由于量级关系导致的无法进行多列的复合计算。数据标准化过程还可以用来解决个别数值较高的属性对聚类结果的影响。

提炼新字段

很多情况下，需要基于业务规则提取新的字段，这些字段也称为复合字段。这些字段通常都是基于单一字段产生，但需要进行复合运算甚至复杂算法模型才能得到新的指标。

属性构造

有些建模过程中，也会需要根据已有的属性集构造新的属性。例如，几乎所有的机器学习都会讲样本分为训练集、测试集、验证集三类，那么数据集的分类（或者叫分区）就属于需要新构建的属性，用户做机器学习不同阶段的样本使用。

提示  在某些场景中，也存在一些特殊转换方法。例如在机器学习中，有些值是离散型的数据但存在一定意义，例如最高学历这个字段中包含博士、研究生、大学、高中这4个值，某些算法不支持直接对文本进行计算，此时需要将学历这个字段进行转换。常见的方法是将值域集中的每个值拆解为一个字段，每个字段取值为0或1（布尔型或数值型）。这时，就会出现4个新的字段，对于一条记录来看（通常是一个人），其最高学历只能满足一个，例如字段博士为1，那么其余的字段（研究生、大学、高中）则为0。因此这个过程实际上是将1个字段根据值域（4个值的集合）拆解为4个字段。

6. 数据压缩

数据压缩是指在保持原有数据集的完整性和准确性，不丢失有用信息的前提下，按照一定的算法和方式对数据进行重新组织的一种技术方法。

对大规模的数据进行复杂的数据分析与数据计算通常需要耗费大量时间，所以在这之前需要进行数据的约减和压缩，减小数据规模，而且还可能面临交互式的数据挖掘，根据数据挖掘前后对比对数据进行信息反馈。这样在精简数据集上进行数据挖掘显然效率更高，并且挖掘出来的结果与使用原有数据集所获得结果基本相同。

数据压缩的意义不止体现在数据计算过程中，还有利于减少存储空间，提高其传输、存储和处理效率，减少数据的冗余和存储的空间，这对于底层大数据平台具有非常重要的意义。

数据压缩有多种方式可供选择：

* 数据聚合：将数据聚合后使用，例如如果汇总全部数据，那么基于更粗粒度的数据更加便利。
* 维度约减：通过相关分析手动消除多余属性，使得参与计算的维度（字段）减少；也可以使用主成分分析、因子分析等进行维度聚合，得到的同样是更少的参与计算的数据维度。
* 数据块消减：利用聚类或参数模型替代原有数据，这种方式常见于多个模型综合进行机器学习和数据挖掘。
* 数据压缩：数据压缩包括无损压缩和有损压缩两种类型。数据压缩常用于磁盘文件、视频、音频、图像等。
***


# 新奇检测Novelty Detection

[http://www.dataivy.cn/blog/%E6%96%B0%E5%A5%87%E6%A3%80%E6%B5%8Bnovelty-detection/?wt_tb=1%7C1503129013144](http://www.dataivy.cn/blog/%E6%96%B0%E5%A5%87%E6%A3%80%E6%B5%8Bnovelty-detection/?wt_tb=1%7C1503129013144)

常用的异常检测方法可分为以下几类：

基于统计的异常检测方法。该方法的基本步骤是对数据点进行建模，再以假定的模型（如泊松分布、正太分布等）根据点的分布来确定是否异常。这种方法首先需要对数据的分布有所了解，进而通过数据变异指标来发现异常数据。常用变异指标有极差、四分位数间距、均差、标准差、变异系数等。但是，基于统计的方法检测出来的异常点产生机制可能不唯一，而且它在很大程度上依赖于待挖掘的数据集是否满足某种概率分布模型，另外模型的参数、离群点的数目等都非常重要，确定这些因素通常都比较困难。因此，实际情况中算法的应用性和可移植性较差。

基于距离的异常检测方法。该方法定义包含并拓展了基于统计的思想，即使数据集不满足任何特定分布模型，它仍能有效地发现离群点，特别是当空间维数比较高时，算法的效率比基于密度的方法要高得多 。算法具体实现时，首先给出记录数据点间的距离（如 曼哈顿距离 、欧氏距离等），然后对数据进行一定的预处理以后就可以根据距离的定义来检测异常值。如基于K-Means的聚类可以将离每个类中心点最远或者不属于任何一个类的数据点提取出来而发现异常值。基于距离的离群检测方法不需要用户拥有任何领域知识且具有比较直观的意义，算法比较容易理解，因此在实际中应用得比较多。

基于密度的离群检测方法。这种方法一般都建立在距离的基础上，其主要思想是将数据点之间的距离和某一范围内数据数这两个参数结合起来，从而得到“密度”的概念，然后根据密度判定记录是否为离群点。例如LOF（局部异常因子）就是用于识别基于密度的局部异常值的算法。离群点被定义为相对于全局的局部离群点，这与传统异常点的定义不同，异常点不再是一个二值属性（要么是异常点，要么是正常点，实际上的定义类似于98%的可能性是一个异常点），它摈弃了以前所有的异常定义中非此即彼的绝对异常观念，更加符合现实生活中的应用；但其缺点就是它只对数值数据有效。

基于偏移的异常点检测方法。基于偏移的离群检测算法 (Deviation-based Outlier Detection) 通过对测试数据集主要特征的检验来发现离群点。目前，基于偏移的检测算法大多都停留在理论研究上，实际应用比较少。

基于时间序列的异常点监测方法。所谓时间序列就是将某一指标在不同时间上的数值，按照时间先后顺序排序而成的数列。这种数列虽然由于受到各种偶然因素的影响而表现出某种随机性，不可能完全准确地用历史值来预测将来，但是前后时刻的数值或数据点的相关性往往呈现某种趋势性或周期性变化，这是时间序列挖掘的可行性之所在。时间序列中没有具体描述被研究现象与其影响因素之间的关系，而是把各影响因素分别看作一种作用力，被研究对象的时间序列则看成合力；然后按作用特点和影响效果将影响因素规为 4 类，即趋势变动（ T ）、季节变动（ S ）、循环变动（ C ）和随机变动（ I ）。这四种类项的变动叠加在一起，形成了实际观测到的时间序列，因而可以通过对这四种变动形式的考察来研究时间系列的变动。目前国际和国内对时间序列相似度的研究提出了许多种解决方法，这些方法主要包括基于直接距离、傅立叶变换、 ARMA 模型参数法、规范变换、时间弯曲模型、界标模型、神经网络、小波变换、规则推导等。 关于时间序列的异常检测应用案例，可参照《统计学在点击流数据中的应用范例——Adobe Analytics异常检测》

异常检测根据原始数据集的不同可分为两类：

1. 新奇检测（Novelty Detection）：新奇检测的前提是已知训练数据集是“纯净”的，未被真正的“噪音”数据或真实的“离群点”污染，然后针对这些数据训练完成之后再对新的数据进行训练以寻找异常数据。
2. 离群点检测（Outlier Detection）：离群点检测的训练数据集则包含“离群点”数据，对这些数据训练完成之后再在新的数据集中寻找异常数据。
本文所讲的是新奇检测，通过Python的sklearn实现，应用算法one-class SVM（一类SVM，总觉得这个翻译很怪），one-class SVM用于新奇检测，它的基本原理是在给定的一组样本中，检测数据集的边界以便于区分新的数据点是否属于该类。它是基于密度检测方法的一种，属于无监督学习算法，拟合过程由于不存在数据类标签，因此只需要输入一个矩阵X即可。

```
#coding:utf-8
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.font_manager
from sklearn import svm
xx, yy = np.meshgrid(np.linspace(-5, 5, 500), np.linspace(-5, 5, 500))
# 生成训练数据
X = 0.3 * np.random.randn(100, 2)
X_train = np.r_[X+2, X-2]
# 生成新用于测试的数据
X = 0.3 * np.random.randn(10, 2)
X_test = np.r_[X + 2, X - 2]
# 模型拟合
clf = svm.OneClassSVM(nu=0.1, kernel="rbf", gamma=0.1)
clf.fit(X_train)
y_pred_train = clf.predict(X_train)
y_pred_test = clf.predict(X_test)
print ("novelty detection result:",y_pred_test)
n_error_train = y_pred_train[y_pred_train == -1].size
n_error_test = y_pred_test[y_pred_test == -1].size
# 在平面中绘制点、线和距离平面最近的向量
Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
plt.title("Novelty Detection")
plt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), 0, 7), cmap=plt.cm.Blues_r)
a = plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors="red")
plt.contourf(xx, yy, Z, levels=[0, Z.max()], colors="orange")
b1 = plt.scatter(X_train[:, 0], X_train[:, 1], c="white")
b2 = plt.scatter(X_test[:, 0], X_test[:, 1], c="green")
plt.axis("tight")
plt.xlim((-5, 5))
plt.ylim((-5, 5))
plt.legend([a.collections[0], b1, b2],
["learned frontier", "training observations",
"new observations", ],
loc="upper left",
prop=matplotlib.font_manager.FontProperties(size=11))
plt.xlabel(
"error train: %d/200 ; errors novel regular: %d/40 ; "
% (n_error_train, n_error_test,))
plt.show()
```

```
新奇检测的应用场景包括：

客户异常识别
信用卡欺诈
贷款审批识别
药物变异识别
恶劣气象预测
网络入侵检测
流量作弊
```

one-class SVM属于SVM的一种，可用于高维数据的异常检测，其基于libsvm。one-class SVM提供了linear、poly、rbf、sigmoid和precomputed可供使用，甚至你可以自定义一个内核算法来调用。one-class SVM本质上还是一种分类，但这种分类与传统的“分类”意义不同，one-class SVM的分类是将数据分为“正常数据”和“异常数据”（用+1和-1表示），而传统的分类是将正常数据按照不同的特征分为几个不同的类别。

# 离群点检测Outlier Detection

[http://www.dataivy.cn/blog/%E7%A6%BB%E7%BE%A4%E7%82%B9%E6%A3%80%E6%B5%8Boutlier-detection/?wt_tb=1%7C1503129013144](http://www.dataivy.cn/blog/%E7%A6%BB%E7%BE%A4%E7%82%B9%E6%A3%80%E6%B5%8Boutlier-detection/?wt_tb=1%7C1503129013144)

离群点检测是异常值检测的一种，其思路与新奇检测一致；区别在于离群点检测的原始观测数据集中已经包含异常值，而新奇检测则不包括。


以下是利用Python中SKlearn机器学习库的EllipticEnvelope实现对离群点的检测。EllipticEnvelope是Sklearn协方差估计中对高斯分布数据集的离群值检验方法，且该方法在高维度下的表现效果欠佳。

```
#coding:utf-8
import numpy as np
from sklearn.covariance import EllipticEnvelope
xx, yy = np.meshgrid(np.linspace(-5, 5, 500), np.linspace(-5, 5, 500))
# 生成训练数据
X = 0.3 * np.random.randn(100, 2)
X_train = np.r_[X+2, X-2]
# 生成新用于测试的数据
X = 0.3 * np.random.randn(10, 2)
X_test = np.r_[X + 2, X - 2]
# 模型拟合
clf = EllipticEnvelope()
clf.fit(X_train)
y_pred_train = clf.predict(X_train)
y_pred_test = clf.predict(X_test)
print ("novelty detection result:",y_pred_test)
```

对比Python的这两种异常值检测方法，当数据集在高斯分布下时，基于鲁棒协方差估计的EmpiricalCovariance效果要好于OneClassSVM算法；但在非高斯分布情况下，OneClassSVM的应用效果要好于EmpiricalCovariance（例如数据集中包含两个核心）

